# Directory: llm_rag/

# -----------------------------
# File: README.md
# -----------------------------
# LLM RAG Pipeline ðŸš€

A **production-grade, modular Retrieval-Augmented Generation (RAG)** system to supercharge Large Language Models (LLMs) with your private documents.

## âœ¨ Features
- ðŸ“„ Upload & chunk documents (.txt, .pdf, .md, .docx)
- ðŸ¤– Generate embeddings using OpenAI or HuggingFace
- ðŸ§  Store & retrieve chunks from vector DB (Chroma, FAISS)
- ðŸ§© Chain-based RAG: retrieve relevant chunks â†’ generate answers
- ðŸ–¥ï¸ Clean CLI and sleek Gradio web UI for querying
- ðŸ› ï¸ Fully modular & extensible Python codebase
- ðŸ” API key entry via `.env` or UI textbox
- ðŸ“¦ Clean project structure, logging, error handling

## âš™ï¸ Setup Instructions

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Harish-coder360/llm-rag-pipeline
   cd llm-rag-pipeline
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure your environment:**
   - Rename `.env.example` â†’ `.env`
   - Add your OpenAI key or other configs

## ðŸš€ Run It

### ðŸ” CLI Mode
```bash
python ingest.py --path ./docs  # optional, loads default dir
python main.py --query "What is retrieval-augmented generation?"
```

### ðŸ–¼ï¸ Web UI Mode
```bash
python app.py  # launches Gradio app
```

## ðŸ§© Extend It
- Swap embeddings or vector DB in `config.py`
- Add file types in `app.py > process_uploaded_file()`
- Hook other LLMs or chunking logic

---

Made with ðŸ’¡ by developers, for developers. Perfect for building document-aware AI assistants.
